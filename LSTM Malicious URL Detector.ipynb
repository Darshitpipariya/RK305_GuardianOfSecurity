{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from string import printable\n\nfrom keras.models import  Model, model_from_json, load_model\nfrom keras import regularizers\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import *\nfrom keras.preprocessing import sequence\nfrom keras.optimizers import Adam\nfrom pathlib import Path\nimport json\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA = '/kaggle/input/new-data-set-33/NEW_MERGE_DataSET_WITH_DROP.csv'\ndf = pd.read_csv(DATA)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.DataFrame()\ndf1['url']=df['url']\ndf1['label']=df['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len=75\nurl_int_tokens = [[printable.index(x) + 1 for x in url if x in printable] for url in df1.url]\nX = sequence.pad_sequences(url_int_tokens, maxlen=max_len)\ntarget = np.array(df1.label)\nprint('Matrix dimensions of X: ', X.shape, 'Vector dimension of target: ', target.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df1[df1['label']==1])/len(df1['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, target_train, target_test = model_selection.train_test_split(X, target, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_layers_dims(model):\n    l_layers = model.layers\n    for i in range(len(l_layers)):\n        print(l_layers[i])\n        print('Input Shape: ', l_layers[i].input_shape, 'Output Shape: ', l_layers[i].output_shape)\n\n\ndef save_model(fileModelJSON,fileWeights):\n    if Path(fileModelJSON).is_file():\n        os.remove(fileModelJSON)\n    json_string = model.to_json()\n    with open(fileModelJSON,'w' ) as f:\n        json.dump(json_string, f)\n    if Path(fileWeights).is_file():\n        os.remove(fileWeights)\n    model.save_weights(fileWeights)\n    \ndef load_model(fileModelJSON,fileWeights):\n    with open(fileModelJSON, 'r') as f:\n        model_json = json.load(f)\n        model = model_from_json(model_json)\n    model.load_weights(fileWeights)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lstm_conv(max_len=75, emb_dim=32, max_vocab_len=100, lstm_output_size=32, W_reg=regularizers.l2(1e-4)):\n    main_input = Input(shape=(max_len,), dtype='int32', name='main_input')\n    emb = Embedding(input_dim=max_vocab_len, output_dim=emb_dim, input_length=max_len,\n                embeddings_regularizer=W_reg)(main_input) \n    emb = Dropout(0.25)(emb)\n\n    conv = Convolution1D(kernel_size=5, filters=256, padding='same')(emb)\n    conv = ReLU()(conv)\n\n    conv = MaxPooling1D(pool_size=4)(conv)\n    conv = Dropout(0.5)(conv)\n\n    lstm = LSTM(lstm_output_size)(conv)\n    lstm = Dropout(0.5)(lstm)\n    \n    output = Dense(1, activation='sigmoid', name='output')(lstm)\n\n    model = Model(inputs=[main_input], outputs=[output])\n    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 25\nbatch_size = 20\n\nmodel = lstm_conv()\nhistory=model.fit(X_train, target_train, epochs, batch_size,validation_split=0.02)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint(history.history.keys())\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(X, target, verbose=1)\n\nprint('\\nFinal Cross-Validation Accuracy', accuracy, '\\n')\nprint_layers_dims(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_HOME = 'data'\nmodel_name = \"1DConvLSTM\"\nsave_model(DATA_HOME + model_name + \".json\", DATA_HOME + model_name + \".h5\")\nmodel = load_model(DATA_HOME + model_name + \".json\", DATA_HOME + model_name + \".h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_layers = model.layers\nweights = l_layers[1].get_weights()\nweights[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_url_mal = \"mizhibuluo.com/kpgv0nhtm\"\ntest_url_benign = \"ubuntulinux.org/server/hyperscale\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len=75\nurl_int_tokens = [[printable.index(x) + 1 for x in url if x in printable]for url in [test_url_mal,test_url_benign]]\nX = sequence.pad_sequences(url_int_tokens, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_proba = model.predict(X, batch_size=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_result(proba):\n    if proba > 0.5:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=[]\nfor i in target_proba:\n    pred.append(print_result(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}